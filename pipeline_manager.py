# src/pipeline_manager.py
import os
import pandas as pd
import concurrent.futures
from datetime import datetime, timedelta
from config import now_vn

# --- Äá»ŠNH NGHÄ¨A ÄÆ¯á»œNG DáºªN TUYá»†T Äá»I ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 
CACHE_DIR = os.path.join(BASE_DIR, "data_cache")      

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# --- INIT ---
HAS_PIPELINE = False
VNFetcher = object 
Exporter = object

try:
    # A. Import VNFetcher
    try:
        from vnstock_pipeline.template.vnstock import VNFetcher
    except ImportError:
        try:
            from vnstock_pipeline.core.fetcher import VNFetcher
        except ImportError:
            try:
                from vnstock_pipeline.core.fetcher import Fetcher as VNFetcher
            except ImportError:
                VNFetcher = object 

    # B. Import Exporter
    try:
        from vnstock_pipeline.core.exporter import Exporter
    except ImportError:
        Exporter = object

    # C. Import Quote
    from vnstock_data import Quote
    
    HAS_PIPELINE = True

except ImportError as e:
    print(f"âš ï¸ [Pipeline] Thiáº¿u thÆ° viá»‡n dá»¯ liá»‡u: {e}")

# ==============================================================================
# 2. CUSTOM SCHEDULER
# ==============================================================================
class SimpleScheduler:
    def __init__(self, fetcher, exporter, max_workers=5, retry_attempts=1, **kwargs):
        self.fetcher = fetcher
        self.exporter = exporter
        self.max_workers = max_workers

    def _process_task(self, ticker, f_kwargs, e_kwargs):
        try:
            # 1. Fetch Data
            df = self.fetcher._vn_call(ticker, **f_kwargs)
            # 2. Export Data
            if df is not None and not df.empty:
                self.exporter.export(df, ticker, **e_kwargs)
            return True, f"{ticker}: OK"
        except Exception as e:
            return False, f"{ticker}: {str(e)}"

    def run(self, tickers, fetcher_kwargs, exporter_kwargs):
        interval = fetcher_kwargs.get('interval', 'Unknown')
        print(f"ðŸš€ [Pipeline] Äang táº£i {len(tickers)} mÃ£ (Khung: {interval})...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_task, t, fetcher_kwargs, exporter_kwargs): t 
                for t in tickers
            }
            
            for future in concurrent.futures.as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    success, msg = future.result()
                except Exception as exc:
                    print(f"âŒ {ticker} Exception: {exc}")
        return "Xong"

Scheduler = SimpleScheduler

# ==============================================================================
# 3. Cáº¤U HÃŒNH PIPELINE
# ==============================================================================

# TÃ¬m class AppCacheFetcher trong src/pipeline_manager.py
# Sá»­a phÆ°Æ¡ng thá»©c _vn_call nhÆ° sau:

class AppCacheFetcher(VNFetcher):
    """Fetcher: Táº£i dá»¯ liá»‡u há»— trá»£ nhiá»u khung thá»i gian"""
    def _vn_call(self, ticker: str, **kwargs) -> pd.DataFrame:
        if not HAS_PIPELINE: return pd.DataFrame()
        
        start = kwargs.get('start')
        end = kwargs.get('end')
        interval = kwargs.get('interval', '1D') 
        
        # --- LOGIC Má»šI: Thá»­ TCBS trÆ°á»›c tiÃªn ---
        try:
            # Æ¯u tiÃªn 1: TCBS (Nhanh vÃ  á»•n Ä‘á»‹nh)
            quote = Quote(source='tcbs', symbol=ticker)
            df = quote.history(start=start, end=end, interval=interval)
        except Exception:
            try:
                # Æ¯u tiÃªn 2: VND (Dá»¯ liá»‡u lá»‹ch sá»­ tá»‘t)
                quote = Quote(source='vnd', symbol=ticker)
                df = quote.history(start=start, end=end, interval=interval)
            except Exception:
                try:
                    # Æ¯u tiÃªn 3: VCI (Dá»± phÃ²ng cuá»‘i cÃ¹ng)
                    quote = Quote(source='vci', symbol=ticker)
                    df = quote.history(start=start, end=end, interval=interval)
                except:
                    return pd.DataFrame()
        
        if df.empty:
            raise ValueError(f"No data for {ticker}")
            
        # ... (Pháº§n chuáº©n hÃ³a cá»™t phÃ­a dÆ°á»›i giá»¯ nguyÃªn) ...
        df = df.rename(columns={
            'time': 'Date', 'open': 'Open', 'high': 'High', 
            'low': 'Low', 'close': 'Close', 'volume': 'Volume'
        })
        
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.set_index('Date')
            
        return df

class ParquetCacheExporter(Exporter):
    """Exporter: LÆ°u file theo format {Symbol}_{Interval}.parquet"""
    def export(self, data: pd.DataFrame, ticker: str, **kwargs):
        output_dir = kwargs.get('output_dir', CACHE_DIR)
        interval = kwargs.get('interval', '1D') 
        
        os.makedirs(output_dir, exist_ok=True)
        file_path = os.path.join(output_dir, f"{ticker}_{interval}.parquet")
        
        if os.path.exists(file_path):
            try:
                df_old = pd.read_parquet(file_path)
                data = pd.concat([df_old, data])
                data = data[~data.index.duplicated(keep='last')]
                data = data.sort_index()
            except: pass
            
        data.to_parquet(file_path)

# ==============================================================================
# 4. HÃ€M CHáº Y CHÃNH (ÄÃƒ FIX Lá»–I THAM Sá»)
# ==============================================================================

def run_bulk_update(tickers_list, days_back=200):
    """
    Cáº­p nháº­t dá»¯ liá»‡u Ä‘a khung thá»i gian:
    - D1: 365 ngÃ y
    - 1H: 50 ngÃ y
    - 15m: 20 ngÃ y
    """
    if not HAS_PIPELINE:
        return "âš ï¸ Lá»—i: ChÆ°a cÃ i Ä‘áº·t thÆ° viá»‡n 'vnstock_data'."
        
    try:
        fetcher = AppCacheFetcher()
        exporter = ParquetCacheExporter()
        scheduler = Scheduler(fetcher=fetcher, exporter=exporter, max_workers=10)
        
        # 1. Láº¥y thá»i gian hiá»‡n táº¡i theo VN Ä‘á»ƒ Ä‘á»“ng nháº¥t
        now = now_vn() 
        end_date = now.strftime('%Y-%m-%d')
        
        # 2. Äá»‹nh nghÄ©a cÃ¡c cáº¥u hÃ¬nh táº£i
        configs = [
            {"label": "D1", "days": days_back, "interval": "1D"},
            {"label": "1H", "days": 30, "interval": "1H"},
            {"label": "15m", "days": 12, "interval": "15m"}
        ]
        
        # 3. Cháº¡y vÃ²ng láº·p cáº­p nháº­t
        for i, cfg in enumerate(configs, 1):
            start_date = (now - timedelta(days=cfg['days'])).strftime('%Y-%m-%d')
            print(f"ðŸ”„ [{i}/3] Äang táº£i {cfg['label']} ({cfg['days']} ngÃ y) | Tá»« {start_date} Ä‘áº¿n {end_date}")
            
            scheduler.run(
                tickers=tickers_list,
                fetcher_kwargs={'start': start_date, 'end': end_date, 'interval': cfg['interval']},
                exporter_kwargs={'output_dir': CACHE_DIR, 'interval': cfg['interval']}
            )
        
        return f"âœ… ÄÃ£ cáº­p nháº­t xong dá»¯ liá»‡u: D1 ({days_back}d), 1H (50d), 15m (20d)."
        
    except Exception as e:
        return f"âŒ Lá»—i Runtime: {str(e)}"

# Cáº­p nháº­t trong pipeline_manager.py
def smart_universe_scan(universe_list, min_vol=100000, min_price=10):
    """
    Tá»‘i Æ°u: 
    1. Chá»‰ update D1 cho toÃ n bá»™ list (Sá»­ dá»¥ng Multi-thread Worker = 20 vÃ¬ D1 nháº¹).
    2. Lá»c danh sÃ¡ch Ä‘áº¡t vol/price.
    3. Tráº£ vá» list 'active_symbols' Ä‘á»ƒ Scanner tiáº¿p tá»¥c xá»­ lÃ½ sÃ¢u (1H/15m).
    """
    from data import load_data_with_cache
    
    active_symbols = []
    
    # B1: QuÃ©t nhanh D1
    print(f"âš¡ Äang lá»c thÃ´ {len(universe_list)} mÃ£...")
    
    # Máº¹o: DÃ¹ng ThreadPoolExecutor Ä‘á»ƒ check Cache/API nhanh
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        # Load cache D1 (ráº¥t nhanh vÃ¬ file parquet Ä‘Ã£ cÃ³ sáºµn 365 ngÃ y, chá»‰ fetch thÃªm 1 ngÃ y)
        future_to_sym = {executor.submit(load_data_with_cache, sym, 50, "1D"): sym for sym in universe_list}
        
        for future in concurrent.futures.as_completed(future_to_sym):
            sym = future_to_sym[future]
            try:
                df = future.result()
                if df is not None and not df.empty:
                    last = df.iloc[-1]
                    avg_vol = df['Volume'].tail(20).mean()
                    # Äiá»u kiá»‡n lá»c Universe
                    if last['Close'] >= min_price and avg_vol >= min_vol:
                        active_symbols.append(sym)
            except:
                pass
                
    return active_symbols