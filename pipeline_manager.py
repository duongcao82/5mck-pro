# src/pipeline_manager.py
import os
import pandas as pd
import concurrent.futures
from datetime import datetime, timedelta
from config import now_vn

# --- ƒê·ªäNH NGHƒ®A ƒê∆Ø·ªúNG D·∫™N TUY·ªÜT ƒê·ªêI ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 
CACHE_DIR = os.path.join(BASE_DIR, "data_cache")      

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# --- INIT ---
HAS_PIPELINE = False
VNFetcher = object 
Exporter = object

try:
    # A. Import VNFetcher
    try:
        from vnstock_pipeline.template.vnstock import VNFetcher
    except ImportError:
        try:
            from vnstock_pipeline.core.fetcher import VNFetcher
        except ImportError:
            try:
                from vnstock_pipeline.core.fetcher import Fetcher as VNFetcher
            except ImportError:
                VNFetcher = object 

    # B. Import Exporter
    try:
        from vnstock_pipeline.core.exporter import Exporter
    except ImportError:
        Exporter = object

    # C. Import Quote
    from vnstock_data import Quote
    
    HAS_PIPELINE = True

except ImportError as e:
    print(f"‚ö†Ô∏è [Pipeline] Thi·∫øu th∆∞ vi·ªán d·ªØ li·ªáu: {e}")

# ==============================================================================
# 2. CUSTOM SCHEDULER
# ==============================================================================
class SimpleScheduler:
    def __init__(self, fetcher, exporter, max_workers=5, retry_attempts=1, **kwargs):
        self.fetcher = fetcher
        self.exporter = exporter
        self.max_workers = max_workers

    def _process_task(self, ticker, f_kwargs, e_kwargs):
        try:
            # 1. Fetch Data
            df = self.fetcher._vn_call(ticker, **f_kwargs)
            # 2. Export Data
            if df is not None and not df.empty:
                self.exporter.export(df, ticker, **e_kwargs)
            return True, f"{ticker}: OK"
        except Exception as e:
            return False, f"{ticker}: {str(e)}"

    def run(self, tickers, fetcher_kwargs, exporter_kwargs):
        interval = fetcher_kwargs.get('interval', 'Unknown')
        print(f"üöÄ [Pipeline] ƒêang t·∫£i {len(tickers)} m√£ (Khung: {interval})...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_task, t, fetcher_kwargs, exporter_kwargs): t 
                for t in tickers
            }
            
            for future in concurrent.futures.as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    success, msg = future.result()
                except Exception as exc:
                    print(f"‚ùå {ticker} Exception: {exc}")
        return "Xong"

Scheduler = SimpleScheduler

# ==============================================================================
# 3. C·∫§U H√åNH PIPELINE
# ==============================================================================

# T√¨m class AppCacheFetcher trong src/pipeline_manager.py
# S·ª≠a ph∆∞∆°ng th·ª©c _vn_call nh∆∞ sau:

class AppCacheFetcher(VNFetcher):
    """Fetcher: T·∫£i d·ªØ li·ªáu h·ªó tr·ª£ nhi·ªÅu khung th·ªùi gian"""
    def _vn_call(self, ticker: str, **kwargs) -> pd.DataFrame:
        if not HAS_PIPELINE: return pd.DataFrame()
        
        start = kwargs.get('start')
        end = kwargs.get('end')
        interval = kwargs.get('interval', '1D') 
        
        # --- LOGIC M·ªöI: Th·ª≠ TCBS tr∆∞·ªõc ti√™n ---
        try:
            # ∆Øu ti√™n 1: TCBS (Nhanh v√† ·ªïn ƒë·ªãnh)
            quote = Quote(source='tcbs', symbol=ticker)
            df = quote.history(start=start, end=end, interval=interval)
        except Exception:
            try:
                # ∆Øu ti√™n 2: VND (D·ªØ li·ªáu l·ªãch s·ª≠ t·ªët)
                quote = Quote(source='vnd', symbol=ticker)
                df = quote.history(start=start, end=end, interval=interval)
            except Exception:
                try:
                    # ∆Øu ti√™n 3: VCI (D·ª± ph√≤ng cu·ªëi c√πng)
                    quote = Quote(source='vci', symbol=ticker)
                    df = quote.history(start=start, end=end, interval=interval)
                except:
                    return pd.DataFrame()
        
        if df.empty:
            raise ValueError(f"No data for {ticker}")
            
        # ... (Ph·∫ßn chu·∫©n h√≥a c·ªôt ph√≠a d∆∞·ªõi gi·ªØ nguy√™n) ...
        df = df.rename(columns={
            'time': 'Date', 'open': 'Open', 'high': 'High', 
            'low': 'Low', 'close': 'Close', 'volume': 'Volume'
        })
        
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.set_index('Date')
            
        return df

class ParquetCacheExporter(Exporter):
    """Exporter: L∆∞u file theo format {Symbol}_{Interval}.parquet"""
    def export(self, data: pd.DataFrame, ticker: str, **kwargs):
        output_dir = kwargs.get('output_dir', CACHE_DIR)
        interval = kwargs.get('interval', '1D') 
        
        os.makedirs(output_dir, exist_ok=True)
        file_path = os.path.join(output_dir, f"{ticker}_{interval}.parquet")
        
        if os.path.exists(file_path):
            try:
                df_old = pd.read_parquet(file_path)
                data = pd.concat([df_old, data])
                data = data[~data.index.duplicated(keep='last')]
                data = data.sort_index()
            except: pass
            
        data.to_parquet(file_path)

# ==============================================================================
# 4. H√ÄM CH·∫†Y CH√çNH (ƒê√É FIX L·ªñI THAM S·ªê)
# ==============================================================================

def run_bulk_update(tickers_list, days_back=365):
    """
    Ch·∫°y c·∫≠p nh·∫≠t d·ªØ li·ªáu.
    [FIX] Th√™m l·∫°i tham s·ªë days_back ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi app.py c≈©.
    Tuy nhi√™n logic b√™n trong v·∫´n tu√¢n th·ªß y√™u c·∫ßu:
    1. D1: 365 ng√†y (ho·∫∑c theo days_back n·∫øu mu·ªën)
    2. 1H: 50 ng√†y
    3. 15m: 20 ng√†y
    """
    if not HAS_PIPELINE:
        return "‚ö†Ô∏è L·ªói: Ch∆∞a c√†i ƒë·∫∑t th∆∞ vi·ªán 'vnstock_data'."
        
    try:
        fetcher = AppCacheFetcher()
        exporter = ParquetCacheExporter()
        scheduler = Scheduler(fetcher=fetcher, exporter=exporter, max_workers=10)
        
        end_date = now_vn().strftime('%Y-%m-%d')        
        # --- [BATCH 1] D·ªÆ LI·ªÜU NG√ÄY (D1) ---
        # S·ª≠ d·ª•ng tham s·ªë days_back ƒë·ªÉ tr√°nh l·ªói g·ªçi h√†m, m·∫∑c ƒë·ªãnh l√† 365
        start_d1 = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        print(f"üîÑ [1/3] B·∫Øt ƒë·∫ßu t·∫£i D1 ({days_back} ng√†y)...")
        scheduler.run(
            tickers=tickers_list,
            fetcher_kwargs={'start': start_d1, 'end': end_date, 'interval': '1D'},
            exporter_kwargs={'output_dir': CACHE_DIR, 'interval': '1D'}
        )

        # --- [BATCH 2] D·ªÆ LI·ªÜU 1 GI·ªú (1H) - 300 Ng√†y (C·ªë ƒë·ªãnh) ---
        start_1h = (datetime.now() - timedelta(days=300)).strftime('%Y-%m-%d')
        print("üîÑ [2/3] B·∫Øt ƒë·∫ßu t·∫£i 1H (300 ng√†y)...")
        scheduler.run(
            tickers=tickers_list,
            fetcher_kwargs={'start': start_1h, 'end': end_date, 'interval': '1H'},
            exporter_kwargs={'output_dir': CACHE_DIR, 'interval': '1H'}
        )

        # --- [BATCH 3] D·ªÆ LI·ªÜU 15 PH√öT (15m) - 20 Ng√†y (C·ªë ƒë·ªãnh) ---
        start_15m = (datetime.now() - timedelta(days=300)).strftime('%Y-%m-%d')
        print("üîÑ [3/3] B·∫Øt ƒë·∫ßu t·∫£i 15m (300 ng√†y)...")
        scheduler.run(
            tickers=tickers_list,
            fetcher_kwargs={'start': start_15m, 'end': end_date, 'interval': '15m'},
            exporter_kwargs={'output_dir': CACHE_DIR, 'interval': '15m'}
        )
        
        return "‚úÖ ƒê√£ c·∫≠p nh·∫≠t xong d·ªØ li·ªáu (D1, 1H, 15m)."
    except Exception as e:
        return f"‚ùå L·ªói Runtime: {str(e)}"

def run_universe_pipeline(tickers_list, days=20):
    """
    Ch·∫°y c·∫≠p nh·∫≠t SI√äU T·ªêC cho Universe.
    - Ch·ªâ t·∫£i D1.
    - Ch·ªâ t·∫£i 20 ng√†y.
    - Max Workers cao (20) ƒë·ªÉ qu√©t nhanh.
    """
    if not HAS_PIPELINE: return False
    
    try:
        fetcher = AppCacheFetcher()
        exporter = ParquetCacheExporter()
        
        # TƒÉng Worker l√™n 20 v√¨ request 1D/20 ng√†y r·∫•t nh·∫π, kh√¥ng s·ª£ ngh·∫Ωn
        scheduler = Scheduler(fetcher=fetcher, exporter=exporter, max_workers=20)
        
        end_date = now_vn().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        print(f"[Pipeline] ‚ö° K√≠ch ho·∫°t Universe Bulk Update: {len(tickers_list)} m√£ (D1, {days} ng√†y)...")
        
        scheduler.run(
            tickers=tickers_list,
            fetcher_kwargs={'start': start_date, 'end': end_date, 'interval': '1D'},
            exporter_kwargs={'output_dir': CACHE_DIR, 'interval': '1D'}
        )
        return True
    except Exception as e:
        print(f"Pipeline Error: {e}")
        return False