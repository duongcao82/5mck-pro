# src/pipeline_manager.py
import os
import time
import math
import pandas as pd
import concurrent.futures
from datetime import datetime, timedelta, date
from config import now_vn

# --- ƒê·ªäNH NGHƒ®A ƒê∆Ø·ªúNG D·∫™N TUY·ªÜT ƒê·ªêI ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 
CACHE_DIR = os.path.join(BASE_DIR, "data_cache")      

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# --- INIT ---
HAS_PIPELINE = False
VNFetcher = object 
Exporter = object

try:
    from vnstock_pipeline.template.vnstock import VNFetcher
    from vnstock_pipeline.core.exporter import Exporter
    HAS_PIPELINE = True
except ImportError:
    try:
        from vnstock_pipeline.core.fetcher import VNFetcher
        from vnstock_pipeline.core.exporter import Exporter
        HAS_PIPELINE = True
    except:
        pass

# ==============================================================================
# 2. CUSTOM SCHEDULER
# ==============================================================================
class SimpleScheduler:
    def __init__(self, fetcher, exporter, max_workers=5):
        self.fetcher = fetcher
        self.exporter = exporter
        self.max_workers = max_workers

    def _process_task(self, ticker, f_kwargs, e_kwargs):
        try:
            df = self.fetcher._vn_call(ticker, **f_kwargs)
            if df is not None and not df.empty:
                self.exporter.export(df, ticker, **e_kwargs)
            return True, f"{ticker}: OK"
        except Exception as e:
            return False, f"{ticker}: {str(e)}"

    def run(self, tickers, fetcher_kwargs, exporter_kwargs):
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_task, t, fetcher_kwargs, exporter_kwargs): t 
                for t in tickers
            }
            for future in concurrent.futures.as_completed(future_to_ticker):
                pass 
        return "Xong"

Scheduler = SimpleScheduler

# ==============================================================================
# 3. C·∫§U H√åNH PIPELINE
# ==============================================================================

class AppCacheFetcher(VNFetcher):
    def _vn_call(self, ticker: str, **kwargs) -> pd.DataFrame:
        try:
            from vnstock_data import Quote
        except Exception:
            return pd.DataFrame()
            
        start = kwargs.get('start')
        end = kwargs.get('end')
        interval = kwargs.get('interval', '1D') 
        
        sources = ['tcbs', 'vnd', 'vci']
        for src in sources:
            try:
                quote = Quote(source=src, symbol=ticker)
                df = quote.history(start=start, end=end, interval=interval)
                if df is not None and not df.empty:
                    df = df.rename(columns={
                        'time': 'Date', 'open': 'Open', 'high': 'High', 
                        'low': 'Low', 'close': 'Close', 'volume': 'Volume'
                    })
                    if 'Date' in df.columns:
                        df['Date'] = pd.to_datetime(df['Date'])
                        df = df.set_index('Date')
                    return df
            except:
                continue
        return pd.DataFrame()

class ParquetCacheExporter(Exporter):
    def export(self, data: pd.DataFrame, ticker: str, **kwargs):
        output_dir = kwargs.get('output_dir', CACHE_DIR)
        interval = kwargs.get('interval', '1D') 
        file_path = os.path.join(output_dir, f"{ticker}_{interval}.parquet")
        
        if os.path.exists(file_path):
            try:
                df_old = pd.read_parquet(file_path)
                data = pd.concat([df_old, data])
                data = data[~data.index.duplicated(keep='last')]
                data = data.sort_index()
            except: pass
            
        data.to_parquet(file_path)

# ==============================================================================
# 4. LOGIC T√çNH NG√ÄY GIAO D·ªäCH TH√îNG MINH (H·ªñ TR·ª¢ KHO·∫¢NG TH·ªúI GIAN)
# ==============================================================================
def get_last_trading_date():
    """
    T√¨m ng√†y giao d·ªãch g·∫ßn nh·∫•t (Tr·ª´ l·ªÖ t·∫øt, cu·ªëi tu·∫ßn).
    """
    candidate = now_vn()
    
    # -----------------------------------------------------------
    # C·∫§U H√åNH L·ªäCH NGH·ªà L·ªÑ (C·∫¨P NH·∫¨T T·∫†I ƒê√ÇY)
    # -----------------------------------------------------------
    
    # 1. Ng√†y l·ªÖ c·ªë ƒë·ªãnh (D∆∞∆°ng l·ªãch) - Ch·ªâ c·∫ßn MM-DD
    FIXED_HOLIDAYS_MMDD = ["01-01", "04-30", "05-01", "09-02"]
    
    # 2. Kho·∫£ng th·ªùi gian ngh·ªâ d√†i (T·∫øt, Gi·ªó t·ªï...) - Format: ("YYYY-MM-DD", "YYYY-MM-DD")
    # V√≠ d·ª•: T·∫øt ·∫§t T·ªµ ngh·ªâ t·ª´ 25/01 ƒë·∫øn h·∫øt 02/02/2025
    HOLIDAY_RANGES = [
        ("2026-01-01", "2026-01-04"), # T·∫øt D∆∞∆°ng
        ("2026-02-14", "2026-02-22"), # T·∫øt √Çm
        ("2026-04-25", "2026-04-27"), # Gi·ªó t·ªï
        ("2026-04-30", "2026-05-03"), # 30/4
        ("2026-08-29", "2026-09-02"), # 2/9
    ]
    # -----------------------------------------------------------

    while True:
        candidate_str = candidate.strftime('%Y-%m-%d')
        mm_dd = candidate.strftime('%m-%d')

        # 1. Check Gi·ªù Giao D·ªãch (N·∫øu l√† H√¥m nay)
        if candidate.date() == now_vn().date():
            if candidate.hour < 15 or (candidate.hour == 15 and candidate.minute < 15):
                candidate -= timedelta(days=1)
                continue

        # 2. Check Cu·ªëi Tu·∫ßn (T7, CN)
        if candidate.weekday() >= 5:
            candidate -= timedelta(days=1)
            continue

        # 3. Check Ng√†y L·ªÖ C·ªë ƒê·ªãnh (01/01, 30/04...)
        if mm_dd in FIXED_HOLIDAYS_MMDD:
            candidate -= timedelta(days=1)
            continue
            
        # 4. Check Kho·∫£ng Ngh·ªâ L·ªÖ (Range)
        is_in_range = False
        for start_str, end_str in HOLIDAY_RANGES:
            if start_str <= candidate_str <= end_str:
                is_in_range = True
                break
        
        if is_in_range:
            candidate -= timedelta(days=1)
            continue

        # N·∫øu kh√¥ng d√≠nh ƒëi·ªÅu ki·ªán n√†o -> ƒê√¢y l√† ng√†y giao d·ªãch
        return candidate.date()

def filter_uptodate_tickers(tickers, interval, target_date_obj):
    """L·ªçc b·ªè m√£ ƒë√£ c√≥ d·ªØ li·ªáu m·ªõi nh·∫•t"""
    needed = []
    skipped = 0
    
    for sym in tickers:
        file_path = os.path.join(CACHE_DIR, f"{sym}_{interval}.parquet")
        if not os.path.exists(file_path):
            needed.append(sym)
            continue
            
        try:
            df = pd.read_parquet(file_path)
            if df.empty:
                needed.append(sym)
                continue
            
            last_date_in_cache = df.index[-1].date()
            if last_date_in_cache >= target_date_obj:
                skipped += 1
            else:
                needed.append(sym)
        except:
            needed.append(sym)
            
    return needed, skipped

# ==============================================================================
# 5. H√ÄM CH·∫†Y CH√çNH
# ==============================================================================

def run_bulk_update(tickers_list, days_back=200):
    if not HAS_PIPELINE:
        return "‚ö†Ô∏è L·ªói: Ch∆∞a c√†i ƒë·∫∑t th∆∞ vi·ªán 'vnstock_data'."
        
    try:
        fetcher = AppCacheFetcher()
        exporter = ParquetCacheExporter()
        scheduler = Scheduler(fetcher=fetcher, exporter=exporter, max_workers=10)
        
        # [QUAN TR·ªåNG] L·∫•y ng√†y giao d·ªãch CHU·∫®N
        target_date = get_last_trading_date()
        target_date_str = target_date.strftime('%Y-%m-%d')
        
        now = now_vn()
        end_date_api = now.strftime('%Y-%m-%d') 
        start_date_d1 = (now - timedelta(days=days_back)).strftime('%Y-%m-%d')
        start_date_intra = (now - timedelta(days=4)).strftime('%Y-%m-%d')

        print(f"üìÖ Ng√†y giao d·ªãch m·ª•c ti√™u: {target_date_str} (H√¥m nay: {now.strftime('%d/%m %H:%M')})")

        # --- B∆Ø·ªöC 1: T·∫¢I D1 (CHECK CACHE) ---
        d1_needed, d1_skipped = filter_uptodate_tickers(tickers_list, '1D', target_date)
        
        if d1_needed:
            print(f"üîÑ [1/3] C·∫ßn t·∫£i D1 cho {len(d1_needed)} m√£ (Skip {d1_skipped} m√£ ƒë√£ ƒë·ªß)...")
            scheduler.run(
                tickers=d1_needed,
                fetcher_kwargs={'start': start_date_d1, 'end': end_date_api, 'interval': '1D'},
                exporter_kwargs={'output_dir': CACHE_DIR, 'interval': '1D'}
            )
        else:
            print(f"‚úÖ [1/3] D1 ƒë√£ ƒë·ªß d·ªØ li·ªáu ƒë·∫øn {target_date_str}. B·ªè qua t·∫£i.")

        # --- B∆Ø·ªöC 2: L·ªåC THANH KHO·∫¢N ---
        valid_tickers = []
        min_price = 5.0        
        min_vol = 50_000       
        # Turnover 10 T·ª∑ (ƒë∆°n v·ªã ngh√¨n ƒë·ªìng)
        min_val = 10_000_000   

        print("üîç [2/3] Check thanh kho·∫£n t·ª´ Cache...")
        for sym in tickers_list:
            try:
                path = os.path.join(CACHE_DIR, f"{sym}_1D.parquet")
                if os.path.exists(path):
                    df = pd.read_parquet(path)
                    if len(df) > 5:
                        last = df.iloc[-1]
                        
                        close = float(last['Close'])
                        vol_avg = float(df['Volume'].tail(5).mean())
                        turnover = close * vol_avg
                        
                        if close > min_price and vol_avg > min_vol and turnover > min_val:
                            valid_tickers.append(sym)
            except: continue
        
        print(f"‚úÖ ƒê√£ l·ªçc: {len(valid_tickers)}/{len(tickers_list)} m√£ ƒë·∫°t chu·∫©n > 10 T·ª∑.")

        # --- B∆Ø·ªöC 3: T·∫¢I INTRADAY (BATCHING + CHECK CACHE) ---
        if valid_tickers:
            h1_needed, h1_skipped = filter_uptodate_tickers(valid_tickers, '1H', target_date)
            
            if h1_needed:
                BATCH_SIZE = 20 
                total = len(h1_needed)
                num_batches = math.ceil(total / BATCH_SIZE)
                
                print(f"üîÑ [3/3] C·∫ßn t·∫£i Intraday cho {total} m√£ (Skip {len(valid_tickers) - total} m√£)...")

                for i in range(0, total, BATCH_SIZE):
                    batch = h1_needed[i : i + BATCH_SIZE]
                    print(f"   üì¶ Batch {(i//BATCH_SIZE)+1}/{num_batches}: T·∫£i {len(batch)} m√£...")
                    
                    scheduler.run(batch, 
                        {'start': start_date_intra, 'end': end_date_api, 'interval': '1H'}, 
                        {'output_dir': CACHE_DIR, 'interval': '1H'})
                    
                    scheduler.run(batch, 
                        {'start': start_date_intra, 'end': end_date_api, 'interval': '15m'}, 
                        {'output_dir': CACHE_DIR, 'interval': '15m'})

                    if i + BATCH_SIZE < total:
                        print("   zzz Ngh·ªâ 10s...") 
                        time.sleep(10)
            else:
                print(f"‚úÖ [3/3] Intraday ƒë√£ ƒë·ªß d·ªØ li·ªáu ƒë·∫øn {target_date_str}. B·ªè qua t·∫£i.")
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ m√£ n√†o ƒë·∫°t chu·∫©n thanh kho·∫£n.")

        return f"‚úÖ Ho√†n t·∫•t! (D1 m·ªõi: {len(d1_needed)}, Intra m·ªõi: {len(h1_needed)})"
        
    except Exception as e:
        return f"‚ùå L·ªói Runtime: {str(e)}"

def run_universe_pipeline(universe_list, days=20):
    return run_bulk_update(universe_list, days_back=days)